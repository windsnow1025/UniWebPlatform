import {v4 as uuidv4} from 'uuid';
import ChatClient from "./ChatClient";
import {ApiTypeModel, ChatResponse, ResponseFile} from "@/lib/chat/ChatResponse";
import {Content, ContentTypeEnum, Message, MessageRoleEnum} from "@/client/nest";
import FileLogic from "@/lib/common/file/FileLogic";
import {handleError} from "@/lib/common/ErrorHandler";

export default class ChatLogic {
  private chatClient: ChatClient;
  static getInitMessages = (): Message[] => [
    {
      id: uuidv4(),
      role: MessageRoleEnum.System,
      contents: [
        {
          type: ContentTypeEnum.Text,
          data: "You are a helpful assistant."
        }
      ],
    },
    {
      id: uuidv4(),
      role: MessageRoleEnum.User,
      contents: [
        {
          type: ContentTypeEnum.Text,
          data: ""
        }
      ],
    }
  ];
  static getEmptyUserMessage = (): Message => ({
    id: uuidv4(),
    role: MessageRoleEnum.User,
    contents: [
      {
        type: ContentTypeEnum.Text,
        data: ""
      }
    ],
  });
  static getEmptyAssistantMessage = (): Message => ({
    id: uuidv4(),
    role: MessageRoleEnum.Assistant,
    contents: [],
  });
  static defaultApiTypeModels: ApiTypeModel[] = [
    {apiType: "", model: "", input: 0, output: 0},
  ];

  constructor() {
    this.chatClient = new ChatClient();
  }

  // For deleting files from storage
  static getFileUrlsFromMessage(message: Message): string[] {
    return message.contents
      .filter(content => content.type === ContentTypeEnum.File)
      .map(content => content.data);
  }

  // For deleting files from storage
  static getFileUrlsFromMessages(messages: Message[]): string[] {
    return messages
      .flatMap(message => ChatLogic.getFileUrlsFromMessage(message));
  }

  // For converting model generated file data to urls
  static async getFileUrls(files: ResponseFile[]): Promise<string[]> {
    if (files.length === 0) {
      return [];
    }

    const base64ToFile = (name: string, data: string, type: string) => {
      const byteCharacters = atob(data);
      const byteNumbers = new Array(byteCharacters.length);
      for (let i = 0; i < byteCharacters.length; i++) {
        byteNumbers[i] = byteCharacters.charCodeAt(i);
      }
      const byteArray = new Uint8Array(byteNumbers);
      return new File([byteArray], name, {type: type});
    };

    const fileObjects = files.map((file) =>
      base64ToFile(file.name, file.data, file.type)
    );

    const fileLogic = new FileLogic();
    return await fileLogic.uploadFiles(fileObjects);
  }

  // For non-stream response
  static createAssistantMessage(
    chatResponse: ChatResponse,
    fileUrls: string[],
  ): Message {
    const contents: Content[] = [];

    if (chatResponse.code) {
      contents.push({
        type: ContentTypeEnum.Code,
        data: chatResponse.code
      });
    }

    if (chatResponse.code_output) {
      contents.push({
        type: ContentTypeEnum.CodeOutput,
        data: chatResponse.code_output
      });
    }

    if (chatResponse.text) {
      contents.push({
        type: ContentTypeEnum.Text,
        data: chatResponse.text
      });
    }

    if (fileUrls && fileUrls.length > 0) {
      fileUrls.forEach(fileUrl => {
        contents.push({
          type: ContentTypeEnum.File,
          data: fileUrl
        });
      });
    }

    return {
      id: uuidv4(),
      role: MessageRoleEnum.Assistant,
      contents: contents,
      thought: chatResponse.thought,
      display: chatResponse.display,
    };
  }

  // For stream response
  static updateMessage(
    messages: Message[],
    index: number,
    chunk: ChatResponse,
    fileUrls: string[]
  ): Message[] {
    const newMessages = [...messages];

    if (index < 0 || index >= newMessages.length) {
      return newMessages;
    }

    // Deep Copy
    const currentMessage = {...newMessages[index]};
    currentMessage.contents = [...currentMessage.contents];

    const appendOrCreateContent = (type: ContentTypeEnum, data: string) => {
      const lastContent = currentMessage.contents[currentMessage.contents.length - 1];
      if (lastContent && lastContent.type === type) {
        lastContent.data += data;
      } else {
        currentMessage.contents.push({type, data});
      }
    };

    if (chunk.code) {
      appendOrCreateContent(ContentTypeEnum.Code, chunk.code);
    }

    if (chunk.code_output) {
      appendOrCreateContent(ContentTypeEnum.CodeOutput, chunk.code_output);
    }

    if (chunk.text) {
      appendOrCreateContent(ContentTypeEnum.Text, chunk.text);
    }

    if (fileUrls && fileUrls.length > 0) {
      fileUrls.forEach(fileUrl => {
        currentMessage.contents.push({
          type: ContentTypeEnum.File,
          data: fileUrl
        });
      });
    }

    if (chunk.thought) {
      currentMessage.thought = (currentMessage.thought || '') + chunk.thought;
    }
    if (chunk.display) {
      currentMessage.display = (currentMessage.display || '') + chunk.display;
    }

    // Replace the message in the array with the updated one
    newMessages[index] = currentMessage;

    return newMessages;
  }

  // For chat config
  static getAllApiTypes(apiModels: ApiTypeModel[]): string[] {
    const apiTypes = apiModels.map(model => model.apiType);
    return Array.from(new Set(apiTypes));
  }

  // For chat config
  static getDefaultApiType(apiModels: ApiTypeModel[]): string {
    return ChatLogic.getAllApiTypes(apiModels)[0];
  }

  // For chat config
  static filterApiTypeModelsByApiType(
    apiTypeModels: ApiTypeModel[], apiType: string
  ): ApiTypeModel[] {
    if (!Array.isArray(apiTypeModels) || !apiTypeModels.length || !apiType) {
      return ChatLogic.defaultApiTypeModels;
    }
    return apiTypeModels
      .filter(apiModel => apiModel.apiType === apiType)
  }

  // For chat config
  static filterDefaultModelByApiType(apiTypeModels: ApiTypeModel[], apiType: string): string {
    return ChatLogic.filterApiTypeModelsByApiType(apiTypeModels, apiType)[0].model;
  }

  // For chat config
  async fetchApiTypeModels(): Promise<ApiTypeModel[]> {
    try {
      return await this.chatClient.fetchApiModels();
    } catch (error) {
      throw new Error("Failed to fetch API Models");
    }
  }

  // For chat request
  private static filterOutboundMessages(messages: Message[]): Message[] {
    return messages.map((msg) => ({
      role: msg.role,
      contents: msg.contents.filter(content =>
        content.type === ContentTypeEnum.Text ||
        content.type === ContentTypeEnum.File
      ),
    }));
  }

  // For chat request
  async nonStreamGenerate(
    messages: Message[],
    api_type: string,
    model: string,
    temperature: number,
    thought: boolean,
    code_execution: boolean,
  ): Promise<ChatResponse> {
    try {
      const filteredMessages = ChatLogic.filterOutboundMessages(messages);
      const content = await this.chatClient.nonStreamGenerate(
        filteredMessages, api_type, model, temperature, thought, code_execution
      );
      if (content.error) {
        throw new Error(content.error);
      }

      return {
        text: content.text,
        code: content.code,
        code_output: content.code_output,
        thought: content.thought,
        files: content.files,
        display: content.display,
      };
    } catch (error) {
      handleError(error, 'Failed to generate non-streaming chat response');
    }
  }

  // For chat request
  async* streamGenerate(
    messages: Message[],
    api_type: string,
    model: string,
    temperature: number,
    thought: boolean,
    code_execution: boolean,
    onOpenCallback?: () => void,
  ): AsyncGenerator<ChatResponse, void, unknown> {
    try {
      const filteredMessages = ChatLogic.filterOutboundMessages(messages);
      const response = this.chatClient.streamGenerate(
        filteredMessages, api_type, model, temperature, thought, code_execution, onOpenCallback
      );

      for await (const chunk of response) {
        if (chunk.error) {
          throw new Error(`chunk.error: ${chunk.error}`);
        }

        yield {
          text: chunk.text,
          code: chunk.code,
          code_output: chunk.code_output,
          thought: chunk.thought,
          files: chunk.files,
          display: chunk.display,
        }
      }
    } catch (error) {
      handleError(error, 'Failed to generate streaming chat response');
    }
  }
}